{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>temperatura media (c)</th>\n",
       "      <th>temperatura minima (c)</th>\n",
       "      <th>temperatura maxima (c)</th>\n",
       "      <th>precipitacao (mm)</th>\n",
       "      <th>final de semana</th>\n",
       "      <th>consumo de cerveja (litros)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>27,3</td>\n",
       "      <td>23,9</td>\n",
       "      <td>32,5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>27,02</td>\n",
       "      <td>24,5</td>\n",
       "      <td>33,5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>24,82</td>\n",
       "      <td>22,4</td>\n",
       "      <td>29,9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>23,98</td>\n",
       "      <td>21,5</td>\n",
       "      <td>28,6</td>\n",
       "      <td>1,2</td>\n",
       "      <td>1</td>\n",
       "      <td>29.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>23,82</td>\n",
       "      <td>21</td>\n",
       "      <td>28,3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>23,78</td>\n",
       "      <td>20,1</td>\n",
       "      <td>30,5</td>\n",
       "      <td>12,2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>24</td>\n",
       "      <td>19,5</td>\n",
       "      <td>33,7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>24,9</td>\n",
       "      <td>19,5</td>\n",
       "      <td>32,8</td>\n",
       "      <td>48,6</td>\n",
       "      <td>0</td>\n",
       "      <td>28.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-01-09</td>\n",
       "      <td>28,2</td>\n",
       "      <td>21,9</td>\n",
       "      <td>34</td>\n",
       "      <td>4,4</td>\n",
       "      <td>0</td>\n",
       "      <td>24.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>26,76</td>\n",
       "      <td>22,1</td>\n",
       "      <td>34,2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         data temperatura media (c) temperatura minima (c)  \\\n",
       "0  2015-01-01                  27,3                   23,9   \n",
       "1  2015-01-02                 27,02                   24,5   \n",
       "2  2015-01-03                 24,82                   22,4   \n",
       "3  2015-01-04                 23,98                   21,5   \n",
       "4  2015-01-05                 23,82                     21   \n",
       "5  2015-01-06                 23,78                   20,1   \n",
       "6  2015-01-07                    24                   19,5   \n",
       "7  2015-01-08                  24,9                   19,5   \n",
       "8  2015-01-09                  28,2                   21,9   \n",
       "9  2015-01-10                 26,76                   22,1   \n",
       "\n",
       "  temperatura maxima (c) precipitacao (mm)  final de semana  \\\n",
       "0                   32,5                 0                0   \n",
       "1                   33,5                 0                0   \n",
       "2                   29,9                 0                1   \n",
       "3                   28,6               1,2                1   \n",
       "4                   28,3                 0                0   \n",
       "5                   30,5              12,2                0   \n",
       "6                   33,7                 0                0   \n",
       "7                   32,8              48,6                0   \n",
       "8                     34               4,4                0   \n",
       "9                   34,2                 0                1   \n",
       "\n",
       "   consumo de cerveja (litros)  \n",
       "0                       25.461  \n",
       "1                       28.972  \n",
       "2                       30.814  \n",
       "3                       29.799  \n",
       "4                       28.900  \n",
       "5                       28.218  \n",
       "6                       29.732  \n",
       "7                       28.397  \n",
       "8                       24.886  \n",
       "9                       37.937  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/Consumo_cerveja.csv\")\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Pr√©-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Remover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Trocar ',' por '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(\",\", \".\",regex=True)\n",
    "df = df.applymap(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Normalizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "for column in list(df):\n",
    "    scaled = scaler.fit_transform(df[column].values.reshape(-1, 1))\n",
    "\n",
    "    df[column] = scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47122149699076044"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.copy(deep=True)\n",
    "\n",
    "y_train = df_train[\"consumo de cerveja (litros)\"].values\n",
    "data_train = df_train.drop([\"consumo de cerveja (litros)\"], axis=1)\n",
    "x_train = data_train.values\n",
    "\n",
    "test = x_train[0].reshape((1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_188 (Dense)            (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 20)                120       \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 171\n",
      "Trainable params: 171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(x_train.shape[1], input_dim=x_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "def soft_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "\n",
    "model.compile(loss='mse', optimizer=\"adam\", metrics=[soft_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 292 samples, validate on 73 samples\n",
      "Epoch 1/100\n",
      "292/292 [==============================] - 1s 4ms/step - loss: 0.2462 - soft_acc: 0.6404 - val_loss: 0.2945 - val_soft_acc: 0.4110\n",
      "Epoch 2/100\n",
      "292/292 [==============================] - 0s 170us/step - loss: 0.2315 - soft_acc: 0.6404 - val_loss: 0.2783 - val_soft_acc: 0.4110\n",
      "Epoch 3/100\n",
      "292/292 [==============================] - 0s 137us/step - loss: 0.2168 - soft_acc: 0.6404 - val_loss: 0.2605 - val_soft_acc: 0.4110\n",
      "Epoch 4/100\n",
      "292/292 [==============================] - 0s 166us/step - loss: 0.2006 - soft_acc: 0.6404 - val_loss: 0.2405 - val_soft_acc: 0.4110\n",
      "Epoch 5/100\n",
      "292/292 [==============================] - 0s 156us/step - loss: 0.1835 - soft_acc: 0.6404 - val_loss: 0.2204 - val_soft_acc: 0.4110\n",
      "Epoch 6/100\n",
      "292/292 [==============================] - 0s 125us/step - loss: 0.1657 - soft_acc: 0.6404 - val_loss: 0.1965 - val_soft_acc: 0.4110\n",
      "Epoch 7/100\n",
      "292/292 [==============================] - 0s 163us/step - loss: 0.1455 - soft_acc: 0.6404 - val_loss: 0.1718 - val_soft_acc: 0.4110\n",
      "Epoch 8/100\n",
      "292/292 [==============================] - 0s 163us/step - loss: 0.1256 - soft_acc: 0.6404 - val_loss: 0.1473 - val_soft_acc: 0.4110\n",
      "Epoch 9/100\n",
      "292/292 [==============================] - 0s 156us/step - loss: 0.1065 - soft_acc: 0.6404 - val_loss: 0.1236 - val_soft_acc: 0.4110\n",
      "Epoch 10/100\n",
      "292/292 [==============================] - 0s 126us/step - loss: 0.0882 - soft_acc: 0.6404 - val_loss: 0.1016 - val_soft_acc: 0.4110\n",
      "Epoch 11/100\n",
      "292/292 [==============================] - 0s 133us/step - loss: 0.0719 - soft_acc: 0.6404 - val_loss: 0.0817 - val_soft_acc: 0.4110\n",
      "Epoch 12/100\n",
      "292/292 [==============================] - 0s 120us/step - loss: 0.0578 - soft_acc: 0.6404 - val_loss: 0.0645 - val_soft_acc: 0.4110\n",
      "Epoch 13/100\n",
      "292/292 [==============================] - 0s 98us/step - loss: 0.0457 - soft_acc: 0.6404 - val_loss: 0.0504 - val_soft_acc: 0.4110\n",
      "Epoch 14/100\n",
      "292/292 [==============================] - 0s 172us/step - loss: 0.0370 - soft_acc: 0.6404 - val_loss: 0.0393 - val_soft_acc: 0.4110\n",
      "Epoch 15/100\n",
      "292/292 [==============================] - 0s 150us/step - loss: 0.0309 - soft_acc: 0.6438 - val_loss: 0.0312 - val_soft_acc: 0.4521\n",
      "Epoch 16/100\n",
      "292/292 [==============================] - 0s 169us/step - loss: 0.0265 - soft_acc: 0.7295 - val_loss: 0.0261 - val_soft_acc: 0.5890\n",
      "Epoch 17/100\n",
      "292/292 [==============================] - 0s 121us/step - loss: 0.0246 - soft_acc: 0.7774 - val_loss: 0.0229 - val_soft_acc: 0.6027\n",
      "Epoch 18/100\n",
      "292/292 [==============================] - 0s 162us/step - loss: 0.0235 - soft_acc: 0.7740 - val_loss: 0.0212 - val_soft_acc: 0.6027\n",
      "Epoch 19/100\n",
      "292/292 [==============================] - 0s 169us/step - loss: 0.0230 - soft_acc: 0.7637 - val_loss: 0.0203 - val_soft_acc: 0.6027\n",
      "Epoch 20/100\n",
      "292/292 [==============================] - 0s 154us/step - loss: 0.0228 - soft_acc: 0.7637 - val_loss: 0.0198 - val_soft_acc: 0.6164\n",
      "Epoch 21/100\n",
      "292/292 [==============================] - 0s 173us/step - loss: 0.0225 - soft_acc: 0.7637 - val_loss: 0.0196 - val_soft_acc: 0.6164\n",
      "Epoch 22/100\n",
      "292/292 [==============================] - 0s 162us/step - loss: 0.0223 - soft_acc: 0.7671 - val_loss: 0.0196 - val_soft_acc: 0.6027\n",
      "Epoch 23/100\n",
      "292/292 [==============================] - 0s 124us/step - loss: 0.0220 - soft_acc: 0.7671 - val_loss: 0.0196 - val_soft_acc: 0.6027\n",
      "Epoch 24/100\n",
      "292/292 [==============================] - 0s 155us/step - loss: 0.0217 - soft_acc: 0.7671 - val_loss: 0.0195 - val_soft_acc: 0.6027\n",
      "Epoch 25/100\n",
      "292/292 [==============================] - 0s 161us/step - loss: 0.0214 - soft_acc: 0.7705 - val_loss: 0.0194 - val_soft_acc: 0.6027\n",
      "Epoch 26/100\n",
      "292/292 [==============================] - 0s 188us/step - loss: 0.0211 - soft_acc: 0.7671 - val_loss: 0.0193 - val_soft_acc: 0.6027\n",
      "Epoch 27/100\n",
      "292/292 [==============================] - 0s 130us/step - loss: 0.0209 - soft_acc: 0.7671 - val_loss: 0.0193 - val_soft_acc: 0.6027\n",
      "Epoch 28/100\n",
      "292/292 [==============================] - 0s 171us/step - loss: 0.0206 - soft_acc: 0.7671 - val_loss: 0.0191 - val_soft_acc: 0.6027\n",
      "Epoch 29/100\n",
      "292/292 [==============================] - 0s 166us/step - loss: 0.0204 - soft_acc: 0.7671 - val_loss: 0.0190 - val_soft_acc: 0.6027\n",
      "Epoch 30/100\n",
      "292/292 [==============================] - 0s 197us/step - loss: 0.0201 - soft_acc: 0.7705 - val_loss: 0.0187 - val_soft_acc: 0.6027\n",
      "Epoch 31/100\n",
      "292/292 [==============================] - 0s 151us/step - loss: 0.0199 - soft_acc: 0.7705 - val_loss: 0.0185 - val_soft_acc: 0.6027\n",
      "Epoch 32/100\n",
      "292/292 [==============================] - 0s 193us/step - loss: 0.0196 - soft_acc: 0.7671 - val_loss: 0.0183 - val_soft_acc: 0.6027\n",
      "Epoch 33/100\n",
      "292/292 [==============================] - 0s 153us/step - loss: 0.0194 - soft_acc: 0.7671 - val_loss: 0.0183 - val_soft_acc: 0.6027\n",
      "Epoch 34/100\n",
      "292/292 [==============================] - 0s 151us/step - loss: 0.0191 - soft_acc: 0.7671 - val_loss: 0.0181 - val_soft_acc: 0.6027\n",
      "Epoch 35/100\n",
      "292/292 [==============================] - 0s 155us/step - loss: 0.0189 - soft_acc: 0.7671 - val_loss: 0.0180 - val_soft_acc: 0.6027\n",
      "Epoch 36/100\n",
      "292/292 [==============================] - 0s 166us/step - loss: 0.0187 - soft_acc: 0.7705 - val_loss: 0.0177 - val_soft_acc: 0.6027\n",
      "Epoch 37/100\n",
      "292/292 [==============================] - 0s 152us/step - loss: 0.0184 - soft_acc: 0.7774 - val_loss: 0.0176 - val_soft_acc: 0.6027\n",
      "Epoch 38/100\n",
      "292/292 [==============================] - 0s 112us/step - loss: 0.0182 - soft_acc: 0.7774 - val_loss: 0.0173 - val_soft_acc: 0.6164\n",
      "Epoch 39/100\n",
      "292/292 [==============================] - 0s 179us/step - loss: 0.0179 - soft_acc: 0.7774 - val_loss: 0.0171 - val_soft_acc: 0.6164\n",
      "Epoch 40/100\n",
      "292/292 [==============================] - 0s 162us/step - loss: 0.0177 - soft_acc: 0.7808 - val_loss: 0.0171 - val_soft_acc: 0.6164\n",
      "Epoch 41/100\n",
      "292/292 [==============================] - 0s 175us/step - loss: 0.0174 - soft_acc: 0.7774 - val_loss: 0.0170 - val_soft_acc: 0.6164\n",
      "Epoch 42/100\n",
      "292/292 [==============================] - 0s 161us/step - loss: 0.0172 - soft_acc: 0.7808 - val_loss: 0.0168 - val_soft_acc: 0.6164\n",
      "Epoch 43/100\n",
      "292/292 [==============================] - 0s 112us/step - loss: 0.0169 - soft_acc: 0.7842 - val_loss: 0.0165 - val_soft_acc: 0.6027\n",
      "Epoch 44/100\n",
      "292/292 [==============================] - 0s 187us/step - loss: 0.0167 - soft_acc: 0.7842 - val_loss: 0.0163 - val_soft_acc: 0.6027\n",
      "Epoch 45/100\n",
      "292/292 [==============================] - 0s 94us/step - loss: 0.0164 - soft_acc: 0.7911 - val_loss: 0.0161 - val_soft_acc: 0.6027\n",
      "Epoch 46/100\n",
      "292/292 [==============================] - 0s 178us/step - loss: 0.0161 - soft_acc: 0.7945 - val_loss: 0.0158 - val_soft_acc: 0.6164\n",
      "Epoch 47/100\n",
      "292/292 [==============================] - 0s 157us/step - loss: 0.0159 - soft_acc: 0.7945 - val_loss: 0.0157 - val_soft_acc: 0.6301\n",
      "Epoch 48/100\n",
      "292/292 [==============================] - 0s 110us/step - loss: 0.0156 - soft_acc: 0.8014 - val_loss: 0.0155 - val_soft_acc: 0.6301\n",
      "Epoch 49/100\n",
      "292/292 [==============================] - 0s 127us/step - loss: 0.0153 - soft_acc: 0.8014 - val_loss: 0.0153 - val_soft_acc: 0.6438\n",
      "Epoch 50/100\n",
      "292/292 [==============================] - 0s 140us/step - loss: 0.0150 - soft_acc: 0.8048 - val_loss: 0.0150 - val_soft_acc: 0.6712\n",
      "Epoch 51/100\n",
      "292/292 [==============================] - 0s 133us/step - loss: 0.0147 - soft_acc: 0.8014 - val_loss: 0.0148 - val_soft_acc: 0.6849\n",
      "Epoch 52/100\n",
      "292/292 [==============================] - 0s 187us/step - loss: 0.0145 - soft_acc: 0.8082 - val_loss: 0.0146 - val_soft_acc: 0.6849\n",
      "Epoch 53/100\n",
      "292/292 [==============================] - 0s 104us/step - loss: 0.0142 - soft_acc: 0.8082 - val_loss: 0.0145 - val_soft_acc: 0.6849\n",
      "Epoch 54/100\n",
      "292/292 [==============================] - 0s 185us/step - loss: 0.0139 - soft_acc: 0.8082 - val_loss: 0.0143 - val_soft_acc: 0.6986\n",
      "Epoch 55/100\n",
      "292/292 [==============================] - 0s 154us/step - loss: 0.0137 - soft_acc: 0.8151 - val_loss: 0.0139 - val_soft_acc: 0.6986\n",
      "Epoch 56/100\n",
      "292/292 [==============================] - 0s 129us/step - loss: 0.0134 - soft_acc: 0.8253 - val_loss: 0.0138 - val_soft_acc: 0.7260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "292/292 [==============================] - 0s 132us/step - loss: 0.0131 - soft_acc: 0.8322 - val_loss: 0.0137 - val_soft_acc: 0.7260\n",
      "Epoch 58/100\n",
      "292/292 [==============================] - 0s 131us/step - loss: 0.0129 - soft_acc: 0.8322 - val_loss: 0.0136 - val_soft_acc: 0.7260\n",
      "Epoch 59/100\n",
      "292/292 [==============================] - 0s 146us/step - loss: 0.0127 - soft_acc: 0.8288 - val_loss: 0.0136 - val_soft_acc: 0.7260\n",
      "Epoch 60/100\n",
      "292/292 [==============================] - 0s 154us/step - loss: 0.0125 - soft_acc: 0.8390 - val_loss: 0.0134 - val_soft_acc: 0.7260\n",
      "Epoch 61/100\n",
      "292/292 [==============================] - 0s 119us/step - loss: 0.0123 - soft_acc: 0.8356 - val_loss: 0.0131 - val_soft_acc: 0.7397\n",
      "Epoch 62/100\n",
      "292/292 [==============================] - 0s 123us/step - loss: 0.0121 - soft_acc: 0.8322 - val_loss: 0.0131 - val_soft_acc: 0.7397\n",
      "Epoch 63/100\n",
      "292/292 [==============================] - 0s 108us/step - loss: 0.0120 - soft_acc: 0.8390 - val_loss: 0.0127 - val_soft_acc: 0.7397\n",
      "Epoch 64/100\n",
      "292/292 [==============================] - 0s 138us/step - loss: 0.0118 - soft_acc: 0.8356 - val_loss: 0.0129 - val_soft_acc: 0.7397\n",
      "Epoch 65/100\n",
      "292/292 [==============================] - 0s 123us/step - loss: 0.0116 - soft_acc: 0.8390 - val_loss: 0.0128 - val_soft_acc: 0.7397\n",
      "Epoch 66/100\n",
      "292/292 [==============================] - 0s 110us/step - loss: 0.0115 - soft_acc: 0.8390 - val_loss: 0.0127 - val_soft_acc: 0.7397\n",
      "Epoch 67/100\n",
      "292/292 [==============================] - 0s 147us/step - loss: 0.0114 - soft_acc: 0.8390 - val_loss: 0.0127 - val_soft_acc: 0.7397\n",
      "Epoch 68/100\n",
      "292/292 [==============================] - 0s 135us/step - loss: 0.0112 - soft_acc: 0.8390 - val_loss: 0.0125 - val_soft_acc: 0.7397\n",
      "Epoch 69/100\n",
      "292/292 [==============================] - 0s 146us/step - loss: 0.0111 - soft_acc: 0.8390 - val_loss: 0.0125 - val_soft_acc: 0.7397\n",
      "Epoch 70/100\n",
      "292/292 [==============================] - 0s 152us/step - loss: 0.0110 - soft_acc: 0.8390 - val_loss: 0.0124 - val_soft_acc: 0.7397\n",
      "Epoch 71/100\n",
      "292/292 [==============================] - 0s 125us/step - loss: 0.0110 - soft_acc: 0.8356 - val_loss: 0.0125 - val_soft_acc: 0.7397\n",
      "Epoch 72/100\n",
      "292/292 [==============================] - 0s 146us/step - loss: 0.0109 - soft_acc: 0.8356 - val_loss: 0.0124 - val_soft_acc: 0.7397\n",
      "Epoch 73/100\n",
      "292/292 [==============================] - 0s 108us/step - loss: 0.0108 - soft_acc: 0.8356 - val_loss: 0.0123 - val_soft_acc: 0.7397\n",
      "Epoch 74/100\n",
      "292/292 [==============================] - 0s 101us/step - loss: 0.0107 - soft_acc: 0.8356 - val_loss: 0.0124 - val_soft_acc: 0.7260\n",
      "Epoch 75/100\n",
      "292/292 [==============================] - 0s 129us/step - loss: 0.0106 - soft_acc: 0.8390 - val_loss: 0.0124 - val_soft_acc: 0.7397\n",
      "Epoch 76/100\n",
      "292/292 [==============================] - 0s 159us/step - loss: 0.0106 - soft_acc: 0.8390 - val_loss: 0.0122 - val_soft_acc: 0.7260\n",
      "Epoch 77/100\n",
      "292/292 [==============================] - 0s 150us/step - loss: 0.0105 - soft_acc: 0.8356 - val_loss: 0.0122 - val_soft_acc: 0.7260\n",
      "Epoch 78/100\n",
      "292/292 [==============================] - 0s 117us/step - loss: 0.0105 - soft_acc: 0.8356 - val_loss: 0.0123 - val_soft_acc: 0.7397\n",
      "Epoch 79/100\n",
      "292/292 [==============================] - 0s 124us/step - loss: 0.0104 - soft_acc: 0.8322 - val_loss: 0.0122 - val_soft_acc: 0.7397\n",
      "Epoch 80/100\n",
      "292/292 [==============================] - 0s 149us/step - loss: 0.0104 - soft_acc: 0.8356 - val_loss: 0.0122 - val_soft_acc: 0.7397\n",
      "Epoch 81/100\n",
      "292/292 [==============================] - 0s 78us/step - loss: 0.0104 - soft_acc: 0.8390 - val_loss: 0.0119 - val_soft_acc: 0.7397\n",
      "Epoch 82/100\n",
      "292/292 [==============================] - 0s 126us/step - loss: 0.0103 - soft_acc: 0.8390 - val_loss: 0.0121 - val_soft_acc: 0.7397\n",
      "Epoch 83/100\n",
      "292/292 [==============================] - 0s 92us/step - loss: 0.0102 - soft_acc: 0.8322 - val_loss: 0.0122 - val_soft_acc: 0.7397\n",
      "Epoch 84/100\n",
      "292/292 [==============================] - 0s 134us/step - loss: 0.0102 - soft_acc: 0.8322 - val_loss: 0.0122 - val_soft_acc: 0.7397\n",
      "Epoch 85/100\n",
      "292/292 [==============================] - 0s 104us/step - loss: 0.0102 - soft_acc: 0.8356 - val_loss: 0.0121 - val_soft_acc: 0.7397\n",
      "Epoch 86/100\n",
      "292/292 [==============================] - 0s 154us/step - loss: 0.0101 - soft_acc: 0.8356 - val_loss: 0.0123 - val_soft_acc: 0.7397\n",
      "Epoch 87/100\n",
      "292/292 [==============================] - 0s 98us/step - loss: 0.0101 - soft_acc: 0.8356 - val_loss: 0.0120 - val_soft_acc: 0.7397\n",
      "Epoch 88/100\n",
      "292/292 [==============================] - 0s 135us/step - loss: 0.0101 - soft_acc: 0.8356 - val_loss: 0.0122 - val_soft_acc: 0.7534\n",
      "Epoch 89/100\n",
      "292/292 [==============================] - 0s 106us/step - loss: 0.0100 - soft_acc: 0.8390 - val_loss: 0.0120 - val_soft_acc: 0.7397\n",
      "Epoch 90/100\n",
      "292/292 [==============================] - 0s 131us/step - loss: 0.0100 - soft_acc: 0.8356 - val_loss: 0.0122 - val_soft_acc: 0.7534\n",
      "Epoch 91/100\n",
      "292/292 [==============================] - 0s 127us/step - loss: 0.0100 - soft_acc: 0.8390 - val_loss: 0.0120 - val_soft_acc: 0.7534\n",
      "Epoch 92/100\n",
      "292/292 [==============================] - 0s 103us/step - loss: 0.0099 - soft_acc: 0.8390 - val_loss: 0.0121 - val_soft_acc: 0.7534\n",
      "Epoch 93/100\n",
      "292/292 [==============================] - 0s 125us/step - loss: 0.0099 - soft_acc: 0.8425 - val_loss: 0.0121 - val_soft_acc: 0.7534\n",
      "Epoch 94/100\n",
      "292/292 [==============================] - 0s 115us/step - loss: 0.0099 - soft_acc: 0.8356 - val_loss: 0.0124 - val_soft_acc: 0.7534\n",
      "Epoch 95/100\n",
      "292/292 [==============================] - 0s 141us/step - loss: 0.0099 - soft_acc: 0.8425 - val_loss: 0.0119 - val_soft_acc: 0.7534\n",
      "Epoch 96/100\n",
      "292/292 [==============================] - 0s 128us/step - loss: 0.0099 - soft_acc: 0.8390 - val_loss: 0.0119 - val_soft_acc: 0.7534\n",
      "Epoch 97/100\n",
      "292/292 [==============================] - 0s 78us/step - loss: 0.0099 - soft_acc: 0.8425 - val_loss: 0.0123 - val_soft_acc: 0.7534\n",
      "Epoch 98/100\n",
      "292/292 [==============================] - 0s 134us/step - loss: 0.0098 - soft_acc: 0.8425 - val_loss: 0.0120 - val_soft_acc: 0.7534\n",
      "Epoch 99/100\n",
      "292/292 [==============================] - 0s 87us/step - loss: 0.0098 - soft_acc: 0.8425 - val_loss: 0.0120 - val_soft_acc: 0.7534\n",
      "Epoch 100/100\n",
      "292/292 [==============================] - 0s 158us/step - loss: 0.0098 - soft_acc: 0.8425 - val_loss: 0.0120 - val_soft_acc: 0.7534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.60315454]], dtype=float32)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=50, verbose=1, shuffle=True, validation_split=0.2)\n",
    "model.predict(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
